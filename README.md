# LLM-NLP
小项目1：Tiny vs Giant
## DistilBERT 微调结果 (SST-2)
- 模型：`distilbert-base-uncased`
- 数据集：`glue/sst2`
- 微调轮数：5 epoch
- Batch Size：8
- 最终准确率：**89.1%**
- Optimizer：AdamW
- 保存路径：`saved_models/distilbert-sst2/`
